{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tf_utils import * \n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Samples 1000 random mnist batches and compares which have the same label\n",
    "# then search for the same amount of wrong labels\n",
    "def mnist_generate_data(batch_size=20000):\n",
    "\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    batch_rxs, batch_rys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "    import numpy as np\n",
    "    new_ys = np.argmax(batch_ys,1) == np.argmax(batch_rys,1)\n",
    "\n",
    "    true_ids = np.where(new_ys == True)[0]\n",
    "    num_ids = true_ids.shape[0]\n",
    "\n",
    "    rxs = batch_rxs[true_ids]\n",
    "    xs = batch_xs[true_ids]\n",
    "    ys = new_ys[true_ids]\n",
    "\n",
    "    false_ids = np.where(new_ys == False)[0]\n",
    "    false_ids = false_ids[:num_ids] #prune\n",
    "\n",
    "    rxs = np.append(rxs, batch_rxs[false_ids], axis=0)\n",
    "    xs = np.append(xs, batch_xs[false_ids], axis=0)\n",
    "    ys = np.append(ys, new_ys[false_ids])\n",
    "\n",
    "    ys = dense_to_one_hot(ys) # New vector\n",
    "    \n",
    "    return rxs, xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.899024\n",
      "0.967095\n",
      "0.978489\n",
      "0.983308\n",
      "0.98433\n",
      "0.987332\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fcb3f98410c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mrxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_generate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4a33d77f8ed3>\u001b[0m in \u001b[0;36mmnist_generate_data\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmnist_generate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mbatch_rxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_rys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/examples/tutorials/mnist/input_data.pyc\u001b[0m in \u001b[0;36mnext_batch\u001b[1;34m(self, batch_size, fake_data)\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[0mperm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_examples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m       \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m       \u001b[1;31m# Start next epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Siamese Neural Network\n",
    "\n",
    "# %% Setup input to the network and true output label.  These are\n",
    "# simply placeholders which we'll fill in later.\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "rx = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "# %% We add a new type of placeholder to denote when we are training.\n",
    "# This will be used to change the way we compute the network during\n",
    "# training/testing.\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "# %% We'll convert our MNIST vector data to a 4-D tensor:\n",
    "# N x W x H x C\n",
    "x_tensor = tf.reshape(x, [-1, 28, 28, 1])\n",
    "tf.image_summary('x', x_tensor)\n",
    "rx_tensor = tf.reshape(rx, [-1, 28, 28, 1])\n",
    "tf.image_summary('rx', rx_tensor)\n",
    "\n",
    "# %% MNIST Convolutional Neural Net with same weights for pretraining\n",
    "\n",
    "\n",
    "# %% Siamese Network\n",
    "with tf.variable_scope('siamese_network') as scope:\n",
    "#with tf.variable_scope(\"unbound_siamese_network_1\") as scope:\n",
    "    \n",
    "\n",
    "    h_1 = lrelu(batch_norm(conv2d(x_tensor, 32, name='conv1'),\n",
    "                           is_training, scope='bn1'), name='lrelu1')\n",
    "    h_2 = lrelu(batch_norm(conv2d(h_1, 64, name='conv2'),\n",
    "                           is_training, scope='bn2'), name='lrelu2')\n",
    "    h_3 = lrelu(batch_norm(conv2d(h_2, 64, name='conv3'),\n",
    "                           is_training, scope='bn3'), name='lrelu3')\n",
    "    h_3_flat = tf.reshape(h_3, [-1, 64 * 4 * 4])\n",
    "\n",
    "#with tf.variable_scope(\"unbound_siamese_network_2\") as scope:\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "#     w1 = tf.get_variable('conv1/w')\n",
    "#     b1 = tf.get_variable('conv1/b')\n",
    "#     w2 = tf.get_variable('conv2/w')\n",
    "#     b2 = tf.get_variable('conv2/b')\n",
    "#     w3 = tf.get_variable('conv3/w')\n",
    "#     b3 = tf.get_variable('conv3/b')\n",
    "    \n",
    "#     tf.histogram_summary('w1',w1)\n",
    "#     tf.histogram_summary('b1',b1)\n",
    "#     tf.histogram_summary('w2',w2)\n",
    "#     tf.histogram_summary('b2',b2)\n",
    "#     tf.histogram_summary('w3',w3)\n",
    "#     tf.histogram_summary('b3',b3)\n",
    "        \n",
    "    rh_1 = lrelu(batch_norm(conv2d(rx_tensor, 32, name='conv1'),\n",
    "                           is_training, scope='bn1'), name='lrelu1')\n",
    "    rh_2 = lrelu(batch_norm(conv2d(rh_1, 64, name='conv2'),\n",
    "                           is_training, scope='bn2'), name='lrelu2')\n",
    "    rh_3 = lrelu(batch_norm(conv2d(rh_2, 64, name='conv3'),\n",
    "                           is_training, scope='bn3'), name='lrelu3')\n",
    "    rh_3_flat = tf.reshape(rh_3, [-1, 64 * 4 * 4])\n",
    "    \n",
    "\n",
    "d_3_flat = tf.abs(h_3_flat - rh_3_flat) # L2\n",
    "h_4 = linear(d_3_flat, 2) # Weight the different parameters\n",
    "y_pred = tf.nn.softmax(h_4)\n",
    "\n",
    "\n",
    "# %% \n",
    "\n",
    "# %% Define loss/eval/training functions\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(y_pred))\n",
    "\n",
    "tf.scalar_summary('loss', cross_entropy)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "tf.scalar_summary('accuracy', accuracy)\n",
    "\n",
    "# %% We now create a new session to actually perform the initialization the\n",
    "# variables:\n",
    "sess = tf.Session()\n",
    "merged = tf.merge_all_summaries()\n",
    "writer = tf.train.SummaryWriter(\"./tmp/mnist_logs\", sess.graph_def)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# %% We'll train in minibatches and report accuracy:\n",
    "n_epochs = 100000\n",
    "for epoch_i in range(n_epochs):\n",
    "            \n",
    "    rxs, xs, ys = mnist_generate_data()\n",
    "    result = sess.run([merged, train_step], feed_dict={x: xs, rx: rxs, y: ys, is_training: True})\n",
    "    \n",
    "    writer.add_summary(result[0], epoch_i)\n",
    "    \n",
    "    if epoch_i % 50 == 0:\n",
    "        rxs, xs, ys = mnist_generate_data()\n",
    "        print(sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                           x: xs,\n",
    "                           rx: rxs, \n",
    "                           y: ys,\n",
    "                           is_training: False\n",
    "                       }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "x,x1,y = mnist_generate_data()\n",
    "plt.imshow(x[1].reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(x1[1].reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist.train.next_batch(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in tf.all_variables():\n",
    "    print i.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('test3') as scope:\n",
    "    w = tf.get_variable('w',[1])\n",
    "    scope.reuse_variables()\n",
    "    w2 = tf.get_variable('w',[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
